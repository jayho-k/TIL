# 05_Hugging Face

## Dataset

### HuggingFace Hub에서 dataset 로딩하기

- HuggingFaces는 Hub에서 다양한 데이터들을 가지고 있으며, 이를 이용할 수 있는 기능을 datasets 패키지로 제공
- load_dataset() 함수는 지정된 이름의 데이터세트를 Hub로 부터 다운로드하거나 또는 Local 파일에서 DatasetDict/Dataset으로 형태로 생성하여 반환(지정된 이름으로 이미 Local에 다운로드 되어 있으면, 다시 다운로드 하지 않음)
- load_dataset() 함수 주요 인자.
  - path: dataset의 path. Hub에서 download시 해당 dataset repository(username/dataset명). Local direcoty의 경우 절대 경로(path/to/directory/with/my/csv/data)
  - name: 특정 데이터세트의 경우 내부적으로 여러개의 서브 데이터 세트를 가지고 있는데, 이 특정 서브 데이터 세트를 지정할때 사용. config name으로 불림
  - split: train/validation/test 구분
- load_dataset() 함수의 인자로 split을 명시하지 않으면 train, validation, test등을 key로 하는 모든 Dataset을 가지는 DatasetDict 객체를 반환. split을 명시하면 명시된 Dataset 객체 반환

```python
from datasets import load_dataset

dataset_dict = load_dataset("imdb") #cornell-movie-review-data/rotten_tomatoes
print(f"dataset_dict type:{type(dataset_dict)}")
print(dataset_dict)

#DatasetDict는 보통 key로 'train', 'validation', 'test'를 가지며 해당 key로 Dataset 객체를 반환
dataset_train = dataset_dict['train'] # key로 접근이 가능
print(f"dataset_train type:{type(dataset_train)}")
print(dataset_train)


# 특정 데이터 세트의 경우(특히 다국어 데이터 세트), 여러개의 서브 데이터 세트를 가지고 있으므로
# 이를 name으로 서브 데이터 세트를 지정해 줘야 하는 경우가 있음. 
minds_dataset_dict = load_dataset(path="PolyAI/minds14", name="fr-FR") # fr-FR
print(minds_dataset_dict)
# 다른 인자가 적용되어야 할 수도 있음. 자세한 사항은 Dataset의 spec을 살펴 봐야 함.
# https://huggingface.co/datasets/Helsinki-NLP/kde4
kde_dataset_dict = load_dataset("Helsinki-NLP/kde4", lang1="en", lang2="fr") #kde4
print(kde_dataset_dict)
print(kde_dataset_dict['train'][0])
```



### Dataset 분리하기

- load_dataset() 함수의 split 인자로 특정 Dataset 분리. split 인자값이 들어가면 반환은 DatasetDict가 아니라 Dataset
- Dataset 자체를 train과 test용 별도의 분리하고자 할 때는 Dataset의 train_test_split() 메소드 호출
  - train_size 인자는 전체 Dataset에서 train Dataset의 크기
  - seed는 분할 수행 시 반영되는 random seed값

```python
dataset_dict = load_dataset("cornell-movie-review-data/rotten_tomatoes")
print(dataset_dict)
dataset_train = load_dataset("cornell-movie-review-data/rotten_tomatoes", split="train")
print(f"dataset_train type:{type(dataset_train)}")
print(dataset_train)

# split에 특정 건수만큼을 지정할 수 있음
dataset_train_small = load_dataset("cornell-movie-review-data/rotten_tomatoes", split="train[:5000]")
print(dataset_train_small)

# Dataset의 train_test_split() 메소드로 Dataset를 train과 test 데이터 세트를 가지는 DatasetDict를 생성 반환
# key로 train과 test가 자동 할당됨. 
split_datasets = dataset_dict["train"].train_test_split(train_size=0.9, seed=20)
print(split_datasets)

# test Dataset을 validation Dataset으로 변경. 아래는 한번 수행하면 test Dataset이 없어지므로 다시 수행하면 오류 발생.  
split_datasets["validation"] = split_datasets.pop("test")
print(split_datasets)
```



### Dataset 의 Meta 정보 확인

- Dataset에서 컬럼정보 및 data값 확인을 위한 속성
  - features: dataset.Features 타입을 반환하며 feature명과 type을 반환
  - column_names: feature명을 list로 반환
  - data: 실제 data값을 반환
- DatasetDict도 column_name, data 속성을 지원. 다만 반환값은 train/validation/test 등의 키를 가지는 Dict 타입임.

```python
tr_dataset= load_dataset("cornell-movie-review-data/rotten_tomatoes", split="train")
print(tr_dataset)
print(tr_dataset.features)
print(tr_dataset.column_names)

#DatasetDict는 features 속성은 지원하지 않지만, column_names 속성은 지원
dataset_dict = load_dataset("cornell-movie-review-data/rotten_tomatoes")
print(dataset_dict.column_names) #features

# Dataset의 실제 데이터를 보려면 data 속성으로 접근 가능. 
print(f"tr_dataset.data type:{type(tr_dataset.data)}")
print(f"tr_dataset.data:{tr_dataset.data}")

#DatasetDict 역시 data 속성을 제공. 다만 dict 형태로 제공되며, train/validation/test key값으로 구성됨
print(type(dataset_dict.data))
print(dataset_dict.data)
```



### Dataset의 Indexing

- Dataset의 [] indexing 제공. [ ] indexing 반환 결과는 key로 컬럼명, value로 단일값 또는 list를 가지는 dict 타임임
- column 명으로 접근 가능. 해당 column(feature)에 해당하는 모든 값을 list로 반환

```python
# 단일 indexing이므로 value로 단일값 반환
print(tr_dataset[0])
print(type(tr_dataset[0]))

# range indexing(Slicing)이므로 value는 단일값이 아닌 list 반환
print(tr_dataset[0:2]) # 0:1

#text column의 전체 데이터 반환
print(tr_dataset['text']) # tr_dataset['text'][0:2]
tr_dataset[0:2]

# 전체 text 데이터에서 2개만 반환
print(tr_dataset['text'][0:2])

# 전체 2개 데이터에서 text key 값 반환
print(tr_dataset[0:2]['text'])
```



### Dataset의 주요 operator

- map(): Dataset의 개별 데이터(또는 배치)를 인자로 들어온 함수가 반환하는 새로운 데이터로 Dataset 변환
- select(): 전체 Dataset에서 지정된 영역만큼의 Dataset 반환
- filter(): Dataset의 개별 데이터(또는 배치)를 인자로 들어온 함수가 반환하는 boolean값에 따라 filtering 수행.
- remove_columns(): 지정된 column들을 삭제하며 삭제될 컬럼명을 list로 입력 받음.
- rename_column(): 변경될 컬럼명을 {'기존 컬럼명': '새로운 컬럼명'} 과 같은 dict 형태로 입력 받음.
- set_format(): Dataset의 포맷을 변경(PyTorch, NumPy, Pandas)

```python
dataset_sample = tr_dataset.select(range(0, 10))
print(type(dataset_sample))
print(dataset_sample)

# 함수의 인자로 들어오는 example은 1건(또는 batch 단위) 레코드임. 
def is_positive(example):
    # boolean값을 반환해야 함. 
    return example["label"] == 1
# label 컬럼값이 1인 데이터만 filtering하여 Dataset으로 반환. filter()함수로 인자로 들어가는 함수는 boolean값을 반환해야 함. 
filtered_dataset  = tr_dataset.filter(is_positive)
print(tr_dataset)
print(filtered_dataset)

# 특정 컬럼명을 삭제한 Dataset을 반환. 여러컬럼을 삭제 시 인자로 list를 넣음. 
dataset_no_label = tr_dataset.remove_columns(["label"])
print(dataset_no_label[0])

# 특정 컬럼명이 변환된 Dataset을 반환. 인자로 dict를 넣음(기존 컬럼명:신규 컬럼명)
dataset_renamed = tr_dataset.rename_columns({"text": "new_text"})
print(dataset_renamed[0])
```



### map() - Dataset의 변환을 위한 주요 메소드

- 다운로드된 원본 Dataset에 Tokenizer를 적용하여 Dataset을 변환하는 주요 역할을 수행
- Dataset에서 미리 토큰화 변환이 적용하였으므로 모델에 입력 시 별도의 토큰화 변환 작업이 불필요.
- 인자로 변환을 수행할 function을 입력받으며 해당 function에서 Dataset의 개별 Record 단위 또는 Batch 단위로 변환 수행
- 주요 인자(filter() 메소드도 비슷한 인자를 가짐)
  - function: 개별 단위 record(또는 batch 단위 record들)마다 수행할 function
  - batched: True/False. Default는 False이며 True이면 batch단위로 여러건의 records들을 function에 전달(False면 1건)
  - batch_size: batched가 True인 경우 batch size(default는 1000)
  - remove_columns: map() 적용 후 삭제될 컬럼명

```python
tr_dataset[0]
import re

# map() function의 인자로 들어가는 example은 dict 값. key로 column명, value로 값을 가짐. 단일값 또는 batch 단위의 값이 들어 갈 수 있음. 
def clean_lower(example):
    # print(f"example inside fn:{example}")
    text = example['text']
    # 마침표등의 특수문자를 제거하고, 숫자/문자만 가지고 소문자 변환
    example["text"] = re.sub(r"[^\w\s]", "", text).lower()
    return example
    
dataset_cleaned = tr_dataset.map(clean_lower)
print(dataset_cleaned)
print(f"### original text:\n{tr_dataset['text'][0]}")
print(f"### cleaned text:\n{dataset_cleaned['text'][0]}")


===
def clean_lower_batch(examples):
    # print(f"examples inside fn:{examples}")
    # map() 인자 batched=True 이므로 examples['text']는 개별 값이 아니라 list임
    cleaned_texts = [
        re.sub(r"[^\w\s]", "", text).lower() for text in examples["text"]
    ]
    # examples['text'] = cleaned_texts
    return {"text": cleaned_texts} # examples

dataset_cleaned_batch = tr_dataset.map(clean_lower_batch, batched=True)
# clean_lower_batch() 함수가 text 컬럼값만 반환하지만, dataset_cleaned_batch는 text와 label 컬럼값 모두를 가지고 있음
print(dataset_cleaned_batch)
print(f"### original text:\n{tr_dataset['text'][0]}")
print(f"### cleaned text:\n{dataset_cleaned_batch['text'][0]}")
 # remove_columns 인자로 map() 결과 반환 Dataset의 지정된 컬럼명을 삭제
dataset_cleaned_batch_removed = tr_dataset.map(clean_lower_batch, batched=True, remove_columns=["label"])
# clean_lower_batch() 함수가 text 컬럼값만 반환하지만, dataset_cleaned_batch는 text와 label 컬럼값 모두를 가지고 있음
print(dataset_cleaned_batch_removed)
print(f"### original text:\n{tr_dataset['text'][0]}")
print(f"### cleaned text:\n{dataset_cleaned_batch_removed['text'][0]}")
```



#### DatasetDict에도 map() 수행 가능

- DatasetDict에서 map()을 수행하여 한꺼번에 모든 Dataset들에 map()을 적용할 수 있음

```python
raw_datasets = load_dataset("cornell-movie-review-data/rotten_tomatoes")
all_datasets = raw_datasets.map(clean_lower_batch, batched=True)
print(all_datasets)
print(f"raw_datasets의 text:{raw_datasets['train'][0:1]['text']}")
print(f"all_datasets의 text:{all_datasets['train'][0:1]['text']}")
```



#### map()의 인자로 batched=True 사용 시 유의 사항

- batched=True는 Dataset과 Tokenizer 연동 시 수행 속도 향상을 위해서 필요한 설정.
- 하지만 batched=True가 설정되면 map() function이 반환하는 output dictionary의 key값들의 건수는 인자로 들어가는 examples의 건수와 같거나 반환하는 ouput dictionary의 모든 key값들의 건수가 모두 동일해야 함. 그렇지 않으면 오류 발생.
- 이러한 문제를 해결하기 위해서 필요에 따라 특정 컬럼 자체를 remove_columns로 삭제해 줘야 함.

```python
from datasets import Dataset

simple_dataset = Dataset.from_dict({"a": [0, 1, 2]})
print(simple_dataset)
# map() function이 반환하는 output dictionary의 key값들의 건수는 인자로 들어가는 examples의 건수와 같거나 
# 아니면, 반환하는 ouput dictionary의 모든 key값들의 건수가 모두 동일해야 함.
# 그렇지 않으면 오류 발생.
def simple_map_fn(example):
    b = example['a'] * 2 #new column with 6 elements: [0, 1, 2, 0, 1, 2]
    return {
        'b': b # example['a']
    }
# 아래는 오류를 발생 시킴. 컬럼들 a와 b는 모두 동일한 갯수의 값을 가져야 하지만, a컬럼은 3개의 값. b컬럼은 6개의 값을 가짐. 
simple_mapped_dataset = simple_dataset.map(simple_map_fn, batched=True)
print(simple_mapped_dataset[:])
# function에 lambda function을 사용할 수 있음. 마찬가지로 오류 발생
simple_dataset.map(lambda batch: {"b": batch["a"] * 2}, batched=True)  # new column with 6 elements: [0, 1, 2, 0, 1, 2]
# map() function이 반환하는 output dictionary의 key값들의 건수는 모두 같아야 함. 
# 만약 그렇게 할 수 없다면 동일하지 않는 key 컬럼은 삭제하여 반환. 여기서는 기존 컬럼인 a를 삭제해야 함.  
def simple_map_fn(examples):
    b = examples["a"] * 2 #new column with 6 elements: [0, 1, 2, 0, 1, 2]
    return {
        'b': b
    }

simple_mapped_dataset = simple_dataset.map(simple_map_fn, batched=True, remove_columns=["a"]) # remove colum
print(simple_mapped_dataset)
```







